2018-10-05 05:04:23 [root] WARNING: 重复处理：https://www.amazon.com/REMOTE-STORE-Replacement-Caravan-Chrysler/dp/B01AME64J4/ref=zg_bs_15735141_81?_encoding=UTF8&psc=1&refRID=23EDN2W812YGD8R5ZT6Z
2018-10-05 05:04:29 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 137, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 217, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 146, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f94bff0b438>: Failed to establish a new connection: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B07GSZCT3K/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f94bff0b438>: Failed to establish a new connection: [Errno 104] Connection reset by peer',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B07GSZCT3K/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f94bff0b438>: Failed to establish a new connection: [Errno 104] Connection reset by peer',)))
2018-10-05 05:04:29 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B07GSZCT3K/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B07GSZCT3K/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:04:29 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:04:29 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:04:33 [root] WARNING: 重复处理：https://www.amazon.com/Sierra-Chevrolet-Silverado-Carpet-Dashboard/dp/B01GSW9UVI/ref=zg_bs_15735551_100?_encoding=UTF8&psc=1&refRID=3831ZSF7R8XCZND5M5HJ
2018-10-05 05:04:40 [root] WARNING: 重复处理：https://www.amazon.com/FH-FH-FB063115-Sports-Fabric-compatible/dp/B01N1JLWH8/ref=zg_bs_15735551_71?_encoding=UTF8&psc=1&refRID=3831ZSF7R8XCZND5M5HJ
2018-10-05 05:04:41 [root] WARNING: 重复处理：https://www.amazon.com/FH-FH-FB030115-Breezy-Covers-Airbag/dp/B017WHDU0U/ref=zg_bs_15735551_91?_encoding=UTF8&psc=1&refRID=3831ZSF7R8XCZND5M5HJ
2018-10-05 05:08:45 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000FW7VU0/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000FW7VU0/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:08:45 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000FW7VU0/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000FW7VU0/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:08:45 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:08:45 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:09:00 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B006JMJR5M/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B006JMJR5M/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:09:01 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B006JMJR5M/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B006JMJR5M/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:09:01 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:09:01 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:09:09 [root] WARNING: 重复处理：https://www.amazon.com/ROK-Straps-ROK-10306-Black-Orange/dp/B00SXJM5MQ/ref=zg_bs_404632011_61?_encoding=UTF8&psc=1&refRID=36B1XQ28DEQ9MWQZH23Y
2018-10-05 05:09:53 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009JKGK6/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009JKGK6/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:09:53 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009JKGK6/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009JKGK6/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:09:53 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:09:53 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:10:05 [root] WARNING: 重复处理：https://www.amazon.com/WD-40-10000-Empty-Plastic-Applicator/dp/B000BXKZ5G/ref=zg_bs_15719191_91?_encoding=UTF8&psc=1&refRID=QAF4WF1WG9QACDM6X90H
2018-10-05 05:10:14 [root] WARNING: 重复处理：https://www.amazon.com/Gojo-Industries-5162-03-Antibacterial-Translucent/dp/B0040ZOMSW/ref=zg_bs_15718971_94?_encoding=UTF8&psc=1&refRID=3QK3E9BC5W9BEHMWTBM6
2018-10-05 05:10:18 [root] WARNING: 重复处理：https://www.amazon.com/Lucas-Oil-10115-Semi-Synthetic-2-Cycle/dp/B0002KKTWC/ref=zg_bs_15719331_100?_encoding=UTF8&psc=1&refRID=79K3XSZ71VZKJQM4J232
2018-10-05 05:10:23 [root] WARNING: 重复处理：https://www.amazon.com/Red-Line-58204-ShockProof-Oil/dp/B000CPI5XW/ref=zg_bs_15719331_91?_encoding=UTF8&psc=1&refRID=79K3XSZ71VZKJQM4J232
2018-10-05 05:11:47 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000M0KA0I/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000M0KA0I/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:11:48 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000M0KA0I/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000M0KA0I/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:11:48 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:11:48 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:00 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B002RL9E1G/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B002RL9E1G/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:01 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B002RL9E1G/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B002RL9E1G/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:01 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:01 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:04 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B076NP9DCT/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B076NP9DCT/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:04 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B076NP9DCT/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B076NP9DCT/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:04 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:04 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:30 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:30 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:30 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:30 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:39 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00EB23SF2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00EB23SF2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:39 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00EB23SF2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00EB23SF2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:15:39 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:15:39 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:16:26 [root] WARNING: 重复处理：https://www.amazon.com/Bondo-928-Red-Cream-Hardener/dp/B000CITG7S/ref=zg_bs_15707071_90?_encoding=UTF8&psc=1&refRID=JTKW8BH7TT36W93AN2NB
2018-10-05 05:33:29 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000VUASMY/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000VUASMY/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:33:29 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000VUASMY/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B000VUASMY/ref=dp_olp_all_mbc?ie=UTF8&condition=all (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:33:29 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:33:29 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:34:10 [root] WARNING: 重复处理：https://www.amazon.com/PowerTye-50151-Cargo-featuring-Adjustable/dp/B0022ZXOE0/ref=zg_bs_15737781_70?_encoding=UTF8&psc=1&refRID=J3MR51T0DDQR4X5GEYHR
2018-10-05 05:34:15 [root] WARNING: 重复处理：https://www.amazon.com/Filler-Wrangler-2-Door-4-Door-2007-2016/dp/B06XXW8F73/ref=zg_bs_318292011_49?_encoding=UTF8&psc=1&refRID=SRSV81V7DN1X35XA79K7
2018-10-05 05:34:43 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00AHIWN78/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00AHIWN78/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:34:43 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00AHIWN78/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00AHIWN78/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:34:43 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:34:43 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:35:01 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Vixen-Horns-Antique-Vintage-VXH1002C/dp/B06XS23744/ref=zg_bs_15736071_85?_encoding=UTF8&psc=1&refRID=CNS9RER36QAP4C9ACW5A>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Vixen-Horns-Antique-Vintage-VXH1002C/dp/B06XS23744/ref=zg_bs_15736071_85?_encoding=UTF8&psc=1&refRID=CNS9RER36QAP4C9ACW5A took longer than 20.0 seconds..
2018-10-05 05:36:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Black-Bolt-Cover-Pickup-Dakota/dp/B00JFM2E3A/ref=zg_bs_318292011_33?_encoding=UTF8&psc=1&refRID=SRSV81V7DN1X35XA79K7>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Black-Bolt-Cover-Pickup-Dakota/dp/B00JFM2E3A/ref=zg_bs_318292011_33?_encoding=UTF8&psc=1&refRID=SRSV81V7DN1X35XA79K7 took longer than 20.0 seconds..
2018-10-05 05:36:19 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/2015-2018-Unpainted-Material-IKON-MOTORSPORTS/dp/B07CG67WQZ/ref=zg_bs_15737061_84?_encoding=UTF8&psc=1&refRID=CA30QJDD39RFCVX6CAW8>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/2015-2018-Unpainted-Material-IKON-MOTORSPORTS/dp/B07CG67WQZ/ref=zg_bs_15737061_84?_encoding=UTF8&psc=1&refRID=CA30QJDD39RFCVX6CAW8 took longer than 20.0 seconds..
2018-10-05 05:37:02 [root] WARNING: 重复处理：https://www.amazon.com/WinCraft-NFL-Unisex-Adult-Unisex-Children-Standard/dp/B0784KHXW7/ref=zg_bs_15710001_62?_encoding=UTF8&refRID=8TJQQ42385WD2360SCEZ
2018-10-05 05:37:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Philips-DE3175-LongerLife-Miniature-Bulb/dp/B0173MKSE0/ref=zg_bs_3094869011_53?_encoding=UTF8&psc=1&refRID=9Y54JZ6699QV6ZW79STC>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Philips-DE3175-LongerLife-Miniature-Bulb/dp/B0173MKSE0/ref=zg_bs_3094869011_53?_encoding=UTF8&psc=1&refRID=9Y54JZ6699QV6ZW79STC took longer than 20.0 seconds..
2018-10-05 05:37:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Rally-Armor-MF12-BAS-BL-Universal-Hardware/dp/B074WD177J/ref=zg_bs_15706651_69?_encoding=UTF8&psc=1&refRID=0BNETRC6X00GZSABKATJ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Rally-Armor-MF12-BAS-BL-Universal-Hardware/dp/B074WD177J/ref=zg_bs_15706651_69?_encoding=UTF8&psc=1&refRID=0BNETRC6X00GZSABKATJ took longer than 20.0 seconds..
2018-10-05 05:38:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Xenon-Headlight-Optimum-Conversion-Kensun/dp/B00QPH80KO/ref=zg_bs_9002280011_100?_encoding=UTF8&psc=1&refRID=5FB7YDH4863G8T1EW4TT>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Xenon-Headlight-Optimum-Conversion-Kensun/dp/B00QPH80KO/ref=zg_bs_9002280011_100?_encoding=UTF8&psc=1&refRID=5FB7YDH4863G8T1EW4TT took longer than 20.0 seconds..
2018-10-05 05:38:20 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009GVYI2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009GVYI2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:38:20 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009GVYI2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B0009GVYI2/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:38:20 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:38:20 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Cayenne-Clear-Side-Marker-Light/dp/B00175PX36/ref=zg_bs_15730321_11?_encoding=UTF8&psc=1&refRID=P88GXBCHK54BDGEB2QQR>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Cayenne-Clear-Side-Marker-Light/dp/B00175PX36/ref=zg_bs_15730321_11?_encoding=UTF8&psc=1&refRID=P88GXBCHK54BDGEB2QQR took longer than 20.0 seconds..
2018-10-05 05:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Philips-VisionPlus-Upgrade-Headlight-Vision/dp/B00U1OLG6W/ref=zg_bs_15736341_87?_encoding=UTF8&psc=1&refRID=9MGTSWV1T2RV9NECWB9F>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-10-05 05:38:41 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Blazer-C6423-Square-Trailer-Light/dp/B0016H0O6G/ref=zg_bs_9002281011_63?_encoding=UTF8&psc=1&refRID=GZ1T1V1G02PRKAE27JG6>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Blazer-C6423-Square-Trailer-Light/dp/B0016H0O6G/ref=zg_bs_9002281011_63?_encoding=UTF8&psc=1&refRID=GZ1T1V1G02PRKAE27JG6 took longer than 20.0 seconds..
2018-10-05 05:38:41 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Gloveleya-Plastic-Urinals-Portable-Hospital/dp/B01HZ966HO/ref=zg_bs_166889011_92?_encoding=UTF8&psc=1&refRID=338SWKQTQZ00C6T78XSW>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Gloveleya-Plastic-Urinals-Portable-Hospital/dp/B01HZ966HO/ref=zg_bs_166889011_92?_encoding=UTF8&psc=1&refRID=338SWKQTQZ00C6T78XSW took longer than 20.0 seconds..
2018-10-05 05:38:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Britax-Allegiance-Convertible-Seat-Prism/dp/B079W46W4L/ref=zg_bs_17726796011_95?_encoding=UTF8&psc=1&refRID=40NHWVMR6F9P9MTHQ4GT>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Britax-Allegiance-Convertible-Seat-Prism/dp/B079W46W4L/ref=zg_bs_17726796011_95?_encoding=UTF8&psc=1&refRID=40NHWVMR6F9P9MTHQ4GT took longer than 20.0 seconds..
2018-10-05 05:38:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Sesame-Street-Framed-Friends-Potty/dp/B004P3YLPS/ref=zg_bs_166889011_81?_encoding=UTF8&psc=1&refRID=338SWKQTQZ00C6T78XSW>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Sesame-Street-Framed-Friends-Potty/dp/B004P3YLPS/ref=zg_bs_166889011_81?_encoding=UTF8&psc=1&refRID=338SWKQTQZ00C6T78XSW took longer than 20.0 seconds..
2018-10-05 05:38:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Caldesene-Baby-Cornstarch-Powder-Oxide/dp/B01IAHY1OK/ref=zg_bs_322266011_96?_encoding=UTF8&psc=1&refRID=GP7WZ3V4FKDVTKS8ZMZG>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Caldesene-Baby-Cornstarch-Powder-Oxide/dp/B01IAHY1OK/ref=zg_bs_322266011_96?_encoding=UTF8&psc=1&refRID=GP7WZ3V4FKDVTKS8ZMZG took longer than 20.0 seconds..
2018-10-05 05:38:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Johnsons-Baby-Powder-Diaper-Rash/dp/B005IHPE42/ref=zg_bs_322266011_79?_encoding=UTF8&psc=1&refRID=GP7WZ3V4FKDVTKS8ZMZG>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Johnsons-Baby-Powder-Diaper-Rash/dp/B005IHPE42/ref=zg_bs_322266011_79?_encoding=UTF8&psc=1&refRID=GP7WZ3V4FKDVTKS8ZMZG took longer than 20.0 seconds..
2018-10-05 05:39:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Aspen-Begin-Shark-Months/dp/B0054YDLBC/ref=zg_bs_17720256011_96?_encoding=UTF8&psc=1&refRID=GMBDHZXDYPJK6GCGS8QY>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Aspen-Begin-Shark-Months/dp/B0054YDLBC/ref=zg_bs_17720256011_96?_encoding=UTF8&psc=1&refRID=GMBDHZXDYPJK6GCGS8QY took longer than 20.0 seconds..
2018-10-05 05:39:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Little-Martins-Drawer-Trimmer-Light/dp/B01IC6RD12/ref=zg_bs_17720257011_99?_encoding=UTF8&psc=1&refRID=XA7X190C7WT5NQVSH0MA>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Little-Martins-Drawer-Trimmer-Light/dp/B01IC6RD12/ref=zg_bs_17720257011_99?_encoding=UTF8&psc=1&refRID=XA7X190C7WT5NQVSH0MA took longer than 20.0 seconds..
2018-10-05 05:39:50 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:39:50 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:39:50 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:39:50 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:40:06 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:40:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dream-Me-Collection-Changing-Espresso/dp/B0030U27L2/ref=zg_bs_166811011_50?_encoding=UTF8&psc=1&refRID=ERSMNE0VEVAVXS41MW4T>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dream-Me-Collection-Changing-Espresso/dp/B0030U27L2/ref=zg_bs_166811011_50?_encoding=UTF8&psc=1&refRID=ERSMNE0VEVAVXS41MW4T took longer than 20.0 seconds..
2018-10-05 05:40:10 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:40:10 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:40:10 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:40:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dekor-Diaper-Plus-Disposal-System/dp/B00005V6C8/ref=zg_bs_166768011_92?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dekor-Diaper-Plus-Disposal-System/dp/B00005V6C8/ref=zg_bs_166768011_92?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T took longer than 20.0 seconds..
2018-10-05 05:40:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Summer-Infant-Ultra-Plush-Changing/dp/B01M0U1HOH/ref=zg_bs_166765011_95?_encoding=UTF8&psc=1&refRID=SM8T7PMFSG8ZTBQS5XXX>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Summer-Infant-Ultra-Plush-Changing/dp/B01M0U1HOH/ref=zg_bs_166765011_95?_encoding=UTF8&psc=1&refRID=SM8T7PMFSG8ZTBQS5XXX took longer than 20.0 seconds..
2018-10-05 05:40:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Party-Bargains-Disposable-Biodegradable-Easy-Tie/dp/B079K52HFV/ref=zg_bs_166768011_91?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Party-Bargains-Disposable-Biodegradable-Easy-Tie/dp/B079K52HFV/ref=zg_bs_166768011_91?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T took longer than 20.0 seconds..
2018-10-05 05:40:34 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:40:34 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com(0)%20', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:40:34 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:40:34 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:40:36 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Ferlin-DB0723-1-BACKPACK-DIAPER-BAG/dp/B01IY3GR8S/ref=zg_bs_166768011_95?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Ferlin-DB0723-1-BACKPACK-DIAPER-BAG/dp/B01IY3GR8S/ref=zg_bs_166768011_95?_encoding=UTF8&psc=1&refRID=XWW3DCGJK8G7REKFSK7T took longer than 20.0 seconds..
2018-10-05 05:41:19 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Wegreeco-Reusable-Hanging-Cloth-Diaper/dp/B01LVZBLBI/ref=zg_bs_6104946011_73?_encoding=UTF8&psc=1&refRID=CY591T1VG9EKPVMTBTB7>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Wegreeco-Reusable-Hanging-Cloth-Diaper/dp/B01LVZBLBI/ref=zg_bs_6104946011_73?_encoding=UTF8&psc=1&refRID=CY591T1VG9EKPVMTBTB7 took longer than 20.0 seconds..
2018-10-05 05:41:28 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Portable-Changing-Cushioning-Lightweight-Baby/dp/B07BH6YPBQ/ref=zg_bs_546128011_79?_encoding=UTF8&psc=1&refRID=33WJQ5SN1GG40AVBDJHQ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Portable-Changing-Cushioning-Lightweight-Baby/dp/B07BH6YPBQ/ref=zg_bs_546128011_79?_encoding=UTF8&psc=1&refRID=33WJQ5SN1GG40AVBDJHQ took longer than 20.0 seconds..
2018-10-05 05:41:28 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Portable-Changing-Pad-Pacifier-Holder/dp/B07C66SH2W/ref=zg_bs_2237474011_81?_encoding=UTF8&psc=1&refRID=J183PZ49XCG09XDG3H1E>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Portable-Changing-Pad-Pacifier-Holder/dp/B07C66SH2W/ref=zg_bs_2237474011_81?_encoding=UTF8&psc=1&refRID=J183PZ49XCG09XDG3H1E took longer than 20.0 seconds..
2018-10-05 05:41:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Ergobaby-Positions-Award-Winning-Ergonomic-Carrier/dp/B06Y65FDNK/ref=zg_bs_166829011_86?_encoding=UTF8&psc=1&refRID=ZCWRKWMSQ26GA6QVBCS4>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Ergobaby-Positions-Award-Winning-Ergonomic-Carrier/dp/B06Y65FDNK/ref=zg_bs_166829011_86?_encoding=UTF8&psc=1&refRID=ZCWRKWMSQ26GA6QVBCS4 took longer than 20.0 seconds..
2018-10-05 05:41:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Ubbi-10152-Changing-Mat-Gray/dp/B07DFT2VCD/ref=zg_bs_2237474011_73?_encoding=UTF8&psc=1&refRID=J183PZ49XCG09XDG3H1E>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Ubbi-10152-Changing-Mat-Gray/dp/B07DFT2VCD/ref=zg_bs_2237474011_73?_encoding=UTF8&psc=1&refRID=J183PZ49XCG09XDG3H1E took longer than 20.0 seconds..
2018-10-05 05:41:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Ansblue-ISOFIX-Latch-Connector-Child/dp/B07GRM88NL/ref=zg_bs_166836011_95?_encoding=UTF8&psc=1&refRID=5082T70C72DDFFCWN7H1>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Ansblue-ISOFIX-Latch-Connector-Child/dp/B07GRM88NL/ref=zg_bs_166836011_95?_encoding=UTF8&psc=1&refRID=5082T70C72DDFFCWN7H1 took longer than 20.0 seconds..
2018-10-05 05:42:36 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Deterrent-Latch-Appliances-Professional-Installation/dp/B07C4FXX5G/ref=zg_bs_166869011_66?_encoding=UTF8&psc=1&refRID=ABT3JEMP3BXW5A03B4F6>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Deterrent-Latch-Appliances-Professional-Installation/dp/B07C4FXX5G/ref=zg_bs_166869011_66?_encoding=UTF8&psc=1&refRID=ABT3JEMP3BXW5A03B4F6 took longer than 20.0 seconds..
2018-10-05 05:42:45 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Sinotop-Netting-Princess-Mosquito-Bedroom/dp/B01DNMHNGU/ref=zg_bs_2237482011_57?_encoding=UTF8&psc=1&refRID=9SE2V7C7HPW8XVV45Y5H>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Sinotop-Netting-Princess-Mosquito-Bedroom/dp/B01DNMHNGU/ref=zg_bs_2237482011_57?_encoding=UTF8&psc=1&refRID=9SE2V7C7HPW8XVV45Y5H took longer than 20.0 seconds..
2018-10-05 05:42:46 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com1', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com1', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:42:47 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com1', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com1', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:42:47 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:42:47 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:43:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/L%C3%8DLL%C3%89baby-CarryOn-Seasons-Toddler-Embossed/dp/B07888PN4D/ref=zg_bs_166829011_97?_encoding=UTF8&psc=1&refRID=ZCWRKWMSQ26GA6QVBCS4>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/L%C3%8DLL%C3%89baby-CarryOn-Seasons-Toddler-Embossed/dp/B07888PN4D/ref=zg_bs_166829011_97?_encoding=UTF8&psc=1&refRID=ZCWRKWMSQ26GA6QVBCS4 took longer than 20.0 seconds..
2018-10-05 05:43:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Mosquito-Net-Opening-Princess-Suitable/dp/B071WDBDH2/ref=zg_bs_2237482011_67?_encoding=UTF8&psc=1&refRID=9SE2V7C7HPW8XVV45Y5H>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Mosquito-Net-Opening-Princess-Suitable/dp/B071WDBDH2/ref=zg_bs_2237482011_67?_encoding=UTF8&psc=1&refRID=9SE2V7C7HPW8XVV45Y5H took longer than 20.0 seconds..
2018-10-05 05:43:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Summer-Infant-Metal-Expansion-Walk-Thru/dp/B00EFB78M8/ref=zg_bs_166868011_81?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Summer-Infant-Metal-Expansion-Walk-Thru/dp/B00EFB78M8/ref=zg_bs_166868011_81?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29 took longer than 20.0 seconds..
2018-10-05 05:43:33 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Anti-mosquito-Students-Dormitory-Blackout-Nacome/dp/B073WT4ZPC/ref=zg_bs_2237482011_65?_encoding=UTF8&refRID=9SE2V7C7HPW8XVV45Y5H>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Anti-mosquito-Students-Dormitory-Blackout-Nacome/dp/B073WT4ZPC/ref=zg_bs_2237482011_65?_encoding=UTF8&refRID=9SE2V7C7HPW8XVV45Y5H took longer than 20.0 seconds..
2018-10-05 05:43:34 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dreambaby-Chelsea-Close-Security-Extensions/dp/B000EIGJQ2/ref=zg_bs_166868011_94?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dreambaby-Chelsea-Close-Security-Extensions/dp/B000EIGJQ2/ref=zg_bs_166868011_94?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29 took longer than 20.0 seconds..
2018-10-05 05:43:35 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Anti-Lost-Safety-Walking-Harness-Backpack/dp/B07B9Z6C3K/ref=zg_bs_2237486011_85?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Anti-Lost-Safety-Walking-Harness-Backpack/dp/B07B9Z6C3K/ref=zg_bs_2237486011_85?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0 took longer than 20.0 seconds..
2018-10-05 05:43:46 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dreambaby-Chelsea-Extra-Close-Security/dp/B001L64LHU/ref=zg_bs_166868011_82?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dreambaby-Chelsea-Extra-Close-Security/dp/B001L64LHU/ref=zg_bs_166868011_82?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29 took longer than 20.0 seconds..
2018-10-05 05:44:00 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Designed-Quality-Handles-Increased-Visibility/dp/B00K2YB61G/ref=zg_bs_2237486011_89?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Designed-Quality-Handles-Increased-Visibility/dp/B00K2YB61G/ref=zg_bs_2237486011_89?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0 took longer than 20.0 seconds..
2018-10-05 05:44:00 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Portable-Children-Highchair-Adjustable-Shoulder/dp/B07G983L93/ref=zg_bs_2237486011_100?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Portable-Children-Highchair-Adjustable-Shoulder/dp/B07G983L93/ref=zg_bs_2237486011_100?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0 took longer than 20.0 seconds..
2018-10-05 05:44:45 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/walking-Harness-2-Anti-lost-Assistant-Backpack/dp/B076FRNWJ4/ref=zg_bs_2237486011_87?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/walking-Harness-2-Anti-lost-Assistant-Backpack/dp/B076FRNWJ4/ref=zg_bs_2237486011_87?_encoding=UTF8&psc=1&refRID=WSHA4JXCC03QDP5045W0 took longer than 20.0 seconds..
2018-10-05 05:44:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Ar4u-Ultimate-Protector-Pack-12-Pre-applied/dp/B01N2UEJCE/ref=zg_bs_166866011_86?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Ar4u-Ultimate-Protector-Pack-12-Pre-applied/dp/B01N2UEJCE/ref=zg_bs_166866011_86?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD took longer than 20.0 seconds..
2018-10-05 05:45:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Little-Llama-months-Hearing-Protection/dp/B01FIRX1MI/ref=zg_bs_723036011_71?_encoding=UTF8&psc=1&refRID=PBK93Z9JA2RSJZ3BC9XD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Little-Llama-months-Hearing-Protection/dp/B01FIRX1MI/ref=zg_bs_723036011_71?_encoding=UTF8&psc=1&refRID=PBK93Z9JA2RSJZ3BC9XD took longer than 20.0 seconds..
2018-10-05 05:45:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/BANZ-Bluetooth-Earmuffs-Hearing-Protection/dp/B07611XJMH/ref=zg_bs_723036011_72?_encoding=UTF8&psc=1&refRID=PBK93Z9JA2RSJZ3BC9XD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/BANZ-Bluetooth-Earmuffs-Hearing-Protection/dp/B07611XJMH/ref=zg_bs_723036011_72?_encoding=UTF8&psc=1&refRID=PBK93Z9JA2RSJZ3BC9XD took longer than 20.0 seconds..
2018-10-05 05:45:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dreambaby-Pressure-Mount-Hallway-Extensions/dp/B000F0PM12/ref=zg_bs_166868011_99?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dreambaby-Pressure-Mount-Hallway-Extensions/dp/B000F0PM12/ref=zg_bs_166868011_99?_encoding=UTF8&psc=1&refRID=9AP56CYNEM0V9JMAQB29 took longer than 20.0 seconds..
2018-10-05 05:45:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dreambaby-L195-Stove-Top-Guard/dp/B001DYI24S/ref=zg_bs_166869011_62?_encoding=UTF8&psc=1&refRID=ABT3JEMP3BXW5A03B4F6>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dreambaby-L195-Stove-Top-Guard/dp/B001DYI24S/ref=zg_bs_166869011_62?_encoding=UTF8&psc=1&refRID=ABT3JEMP3BXW5A03B4F6 took longer than 20.0 seconds..
2018-10-05 05:45:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/M2cbridge-Furniture-Protectors-Safety-Bumper/dp/B01AAAQ2PG/ref=zg_bs_166866011_95?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/M2cbridge-Furniture-Protectors-Safety-Bumper/dp/B01AAAQ2PG/ref=zg_bs_166866011_95?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD took longer than 20.0 seconds..
2018-10-05 05:45:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Proofing-Protector-Furniture-Padding-Simplife/dp/B07BWF6RHW/ref=zg_bs_166866011_84?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Proofing-Protector-Furniture-Padding-Simplife/dp/B07BWF6RHW/ref=zg_bs_166866011_84?_encoding=UTF8&psc=1&refRID=SFRS9HF867GW54D2AWVD took longer than 20.0 seconds..
2018-10-05 05:45:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Travel-Bug-Toddler-Backpack-Removable/dp/B07DXRV1PD/ref=zg_bs_2237486011_74?_encoding=UTF8&psc=1&refRID=35HR3TD9YS1A3NWY1695>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Travel-Bug-Toddler-Backpack-Removable/dp/B07DXRV1PD/ref=zg_bs_2237486011_74?_encoding=UTF8&psc=1&refRID=35HR3TD9YS1A3NWY1695 took longer than 20.0 seconds..
2018-10-05 05:45:19 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Einstein-60419-Caterpillar-Discovery/dp/B00UZBAEBY/ref=zg_bs_322268011_81?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Einstein-60419-Caterpillar-Discovery/dp/B00UZBAEBY/ref=zg_bs_322268011_81?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:45:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Fisher-Price-Moonlight-Meadow-Deluxe-Play/dp/B00T8VQ46Q/ref=zg_bs_196609011_56?_encoding=UTF8&psc=1&refRID=383D9WB3QT42PGNCQQFK>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Fisher-Price-Moonlight-Meadow-Deluxe-Play/dp/B00T8VQ46Q/ref=zg_bs_196609011_56?_encoding=UTF8&psc=1&refRID=383D9WB3QT42PGNCQQFK took longer than 20.0 seconds..
2018-10-05 05:45:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Newborn-Wireless-Temperature-Lullabies-Available/dp/B0798J1FKR/ref=zg_bs_166870011_82?_encoding=UTF8&psc=1&refRID=GDETQ2QK43JJWYBER9RZ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Newborn-Wireless-Temperature-Lullabies-Available/dp/B0798J1FKR/ref=zg_bs_166870011_82?_encoding=UTF8&psc=1&refRID=GDETQ2QK43JJWYBER9RZ took longer than 20.0 seconds..
2018-10-05 05:46:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Safety-Bedrail-Falling-Toddler-inches/dp/B07BCZXTZY/ref=zg_bs_166872011_80?_encoding=UTF8&psc=1&refRID=7SXNKP22T63RHA0TCXYY>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Safety-Bedrail-Falling-Toddler-inches/dp/B07BCZXTZY/ref=zg_bs_166872011_80?_encoding=UTF8&psc=1&refRID=7SXNKP22T63RHA0TCXYY took longer than 20.0 seconds..
2018-10-05 05:46:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dreambaby-Bath-Soft-Spout-Cover/dp/B000MSVWFM/ref=zg_bs_166864011_80?_encoding=UTF8&psc=1&refRID=JM6WS4357ZSZD46EZ7A7>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dreambaby-Bath-Soft-Spout-Cover/dp/B000MSVWFM/ref=zg_bs_166864011_80?_encoding=UTF8&psc=1&refRID=JM6WS4357ZSZD46EZ7A7 took longer than 20.0 seconds..
2018-10-05 05:46:32 [root] WARNING: 重复处理：https://www.amazon.com/Personalized-Story-Book-Dinkleboo-Daughter/dp/B07BN628SG/ref=zg_bs_239228011_66?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q
2018-10-05 05:46:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/EPLAZA-Toddler-Walking-Penguin-Backpack/dp/B06ZZP37J5/ref=zg_bs_322268011_93?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/EPLAZA-Toddler-Walking-Penguin-Backpack/dp/B06ZZP37J5/ref=zg_bs_322268011_93?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:46:37 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Graco-Oasis-Soothe-Surround-Swing/dp/B01BGVLQCC/ref=zg_bs_166850011_99?_encoding=UTF8&psc=1&refRID=4XFRHPJ940P2ZMHTGBZC>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Graco-Oasis-Soothe-Surround-Swing/dp/B01BGVLQCC/ref=zg_bs_166850011_99?_encoding=UTF8&psc=1&refRID=4XFRHPJ940P2ZMHTGBZC took longer than 20.0 seconds..
2018-10-05 05:46:37 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Sorelle-Vista-Elite-Toddler-Guard/dp/B010R34DIY/ref=zg_bs_166872011_66?_encoding=UTF8&psc=1&refRID=7SXNKP22T63RHA0TCXYY>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Sorelle-Vista-Elite-Toddler-Guard/dp/B010R34DIY/ref=zg_bs_166872011_66?_encoding=UTF8&psc=1&refRID=7SXNKP22T63RHA0TCXYY took longer than 20.0 seconds..
2018-10-05 05:47:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Newborn-Toddler-Anti-slip-Sneakers-0-6Month/dp/B0773PFBD8/ref=zg_bs_322268011_87?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Newborn-Toddler-Anti-slip-Sneakers-0-6Month/dp/B0773PFBD8/ref=zg_bs_322268011_87?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:47:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Household-Essentials-Vision-Storage-Chest/dp/B07712B6C3/ref=zg_bs_166779011_98?_encoding=UTF8&refRID=PF8KJEZM5VSA69NHJSYZ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Household-Essentials-Vision-Storage-Chest/dp/B07712B6C3/ref=zg_bs_166779011_98?_encoding=UTF8&refRID=PF8KJEZM5VSA69NHJSYZ took longer than 20.0 seconds..
2018-10-05 05:47:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Boppy-Pillow-Slipcover-Boutique-Chevron/dp/B018B8P6R4/ref=zg_bs_166798011_83?_encoding=UTF8&psc=1&refRID=MSBZFC8PXT3G6AFZE7ED>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Boppy-Pillow-Slipcover-Boutique-Chevron/dp/B018B8P6R4/ref=zg_bs_166798011_83?_encoding=UTF8&psc=1&refRID=MSBZFC8PXT3G6AFZE7ED took longer than 20.0 seconds..
2018-10-05 05:47:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Nursing-Danha-Breastfeeding-Stretchy-registry/dp/B0167HHIOK/ref=zg_bs_166798011_87?_encoding=UTF8&psc=1&refRID=GN420WA4EMRGYMTQS59P>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Nursing-Danha-Breastfeeding-Stretchy-registry/dp/B0167HHIOK/ref=zg_bs_166798011_87?_encoding=UTF8&psc=1&refRID=GN420WA4EMRGYMTQS59P took longer than 20.0 seconds..
2018-10-05 05:47:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Nook-Sleep-Systems-23559-LilyPad/dp/B00DL8UZYY/ref=zg_bs_196609011_94?_encoding=UTF8&psc=1&refRID=383D9WB3QT42PGNCQQFK>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Nook-Sleep-Systems-23559-LilyPad/dp/B00DL8UZYY/ref=zg_bs_196609011_94?_encoding=UTF8&psc=1&refRID=383D9WB3QT42PGNCQQFK took longer than 20.0 seconds..
2018-10-05 05:47:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Handheld-Protective-Harnesses-Learning-Assistant/dp/B00OKNQBK6/ref=zg_bs_322268011_92?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Handheld-Protective-Harnesses-Learning-Assistant/dp/B00OKNQBK6/ref=zg_bs_322268011_92?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:47:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Dr-Browns-Original-Wide-Neck-Nipple/dp/B015UICLO4/ref=zg_bs_166779011_99?_encoding=UTF8&psc=1&refRID=PF8KJEZM5VSA69NHJSYZ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Dr-Browns-Original-Wide-Neck-Nipple/dp/B015UICLO4/ref=zg_bs_166779011_99?_encoding=UTF8&psc=1&refRID=PF8KJEZM5VSA69NHJSYZ took longer than 20.0 seconds..
2018-10-05 05:47:45 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Gerber-Purees-Organic-Veggie-Variety/dp/B07CTRSGKT/ref=zg_bs_16323111_93?_encoding=UTF8&psc=1&refRID=CN3ETK9FVVSJYS4AZMSG>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Gerber-Purees-Organic-Veggie-Variety/dp/B07CTRSGKT/ref=zg_bs_16323111_93?_encoding=UTF8&psc=1&refRID=CN3ETK9FVVSJYS4AZMSG took longer than 20.0 seconds..
2018-10-05 05:47:46 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Fanatic-Pre-Walker-Washington-Redskins/dp/B00JG7ME5W/ref=zg_bs_322268011_100?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Fanatic-Pre-Walker-Washington-Redskins/dp/B00JG7ME5W/ref=zg_bs_322268011_100?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:47:46 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/HaloVa-Basket-type-Walk-learning-Comfortable-Fall-protecting/dp/B07BS8Y4NZ/ref=zg_bs_322268011_77?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/HaloVa-Basket-type-Walk-learning-Comfortable-Fall-protecting/dp/B07BS8Y4NZ/ref=zg_bs_322268011_77?_encoding=UTF8&psc=1&refRID=HR383B43DMSK8DMKEZSD took longer than 20.0 seconds..
2018-10-05 05:48:16 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/OXO-Tot-Flippy-Snack-Travel/dp/B079KDYTVT/ref=zg_bs_379027011_91?_encoding=UTF8&psc=1&refRID=4QQCXSC9J9PXNCAHMDVC>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/OXO-Tot-Flippy-Snack-Travel/dp/B079KDYTVT/ref=zg_bs_379027011_91?_encoding=UTF8&psc=1&refRID=4QQCXSC9J9PXNCAHMDVC took longer than 20.0 seconds..
2018-10-05 05:48:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Graco-Aire4-Travel-System-Emory/dp/B07B7RX4PZ/ref=zg_bs_166849011_97?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Graco-Aire4-Travel-System-Emory/dp/B07B7RX4PZ/ref=zg_bs_166849011_97?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR took longer than 20.0 seconds..
2018-10-05 05:48:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Britax-B-Free-B-Safe-Travel-System/dp/B07CJNPMKF/ref=zg_bs_166849011_86?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Britax-B-Free-B-Safe-Travel-System/dp/B07CJNPMKF/ref=zg_bs_166849011_86?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR took longer than 20.0 seconds..
2018-10-05 05:48:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Graco-Cruiser-Travel-System-Conrad/dp/B079PC4VX6/ref=zg_bs_166849011_89?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Graco-Cruiser-Travel-System-Conrad/dp/B079PC4VX6/ref=zg_bs_166849011_89?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR took longer than 20.0 seconds..
2018-10-05 05:48:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Happy-Baby-Organic-Creamies-Freeze-Dried/dp/B00975HY8U/ref=zg_bs_16323111_86?_encoding=UTF8&psc=1&refRID=CN3ETK9FVVSJYS4AZMSG>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Happy-Baby-Organic-Creamies-Freeze-Dried/dp/B00975HY8U/ref=zg_bs_16323111_86?_encoding=UTF8&psc=1&refRID=CN3ETK9FVVSJYS4AZMSG took longer than 20.0 seconds..
2018-10-05 05:48:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Britax-Agile-Elite-Travel-System/dp/B075MGHW2L/ref=zg_bs_166849011_90?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Britax-Agile-Elite-Travel-System/dp/B075MGHW2L/ref=zg_bs_166849011_90?_encoding=UTF8&psc=1&refRID=NFS5VNA8JHVF5WZS5RGR took longer than 20.0 seconds..
2018-10-05 05:48:42 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00MNZ0POG/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00MNZ0POG/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:48:43 [analysis_log] ERROR: into_Sellers_list
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 555, in urlopen
    self._prepare_proxy(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 753, in _prepare_proxy
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 230, in connect
    self._tunnel()
  File "/usr/lib/python3.5/http/client.py", line 827, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00MNZ0POG/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 300, in into_Sellers_list
    sellers_list_responses = myrequest_get(url=sellers_list_url,headers=headers)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 256, in myrequest_get
    timeout=20
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.amazon.com', port=443): Max retries exceeded with url: /gp/offer-listing/B00MNZ0POG/ref=dp_olp_new_mbc?ie=UTF8&condition=new (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))
2018-10-05 05:48:43 [analysis_log] ERROR: sellers_list_responses
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1067, in Available
    sellers_list_responses = into_Sellers_list(sellers_list_url)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/analysis_request_header.py", line 305, in into_Sellers_list
    return sellers_list_responses
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:48:43 [analysis_log] ERROR: USA_USA_second_Analysis
Traceback (most recent call last):
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/use_analysis.py", line 79, in USA_analysis
    USA_second_Analysis(ASIN=ASIN,stock=Stock,sellers_list_url=sellers_list_url,product_info=product_info,offerListingID=offerListingID)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1198, in USA_second_Analysis
    usa_second_product_obj.Available(sellers_list_url=sellers_list_url,ASIN=ASIN)
  File "/root/Xuggest_scrapy/AMA_ANALYSIS/USA_analysis.py", line 1072, in Available
    etree_html = etree.HTML(sellers_list_responses)
UnboundLocalError: local variable 'sellers_list_responses' referenced before assignment
2018-10-05 05:49:05 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/MICHEF-Suction-Perfect-Toddlers-Approved/dp/B0794NYXH5/ref=zg_bs_617767011_100?_encoding=UTF8&psc=1&refRID=TP5Y8BXHHKXZJF25KAFN>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/MICHEF-Suction-Perfect-Toddlers-Approved/dp/B0794NYXH5/ref=zg_bs_617767011_100?_encoding=UTF8&psc=1&refRID=TP5Y8BXHHKXZJF25KAFN took longer than 20.0 seconds..
2018-10-05 05:49:05 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/BABYBJORN-Soft-Bib-Pink-Green/dp/B0034GGCY0/ref=zg_bs_7874755011_96?_encoding=UTF8&psc=1&refRID=VQMT2C06Z21E7S2KCXSX>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/BABYBJORN-Soft-Bib-Pink-Green/dp/B0034GGCY0/ref=zg_bs_7874755011_96?_encoding=UTF8&psc=1&refRID=VQMT2C06Z21E7S2KCXSX took longer than 20.0 seconds..
2018-10-05 05:49:16 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/BRITAX-U711901-Britax-B-Free-Stroller/dp/B076DBM5MW/ref=zg_bs_166845011_85?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/BRITAX-U711901-Britax-B-Free-Stroller/dp/B076DBM5MW/ref=zg_bs_166845011_85?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM took longer than 20.0 seconds..
2018-10-05 05:49:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Jhua-Password-Electronic-Children-Christmas/dp/B075YT4RX3/ref=zg_bs_2237480011_62?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Jhua-Password-Electronic-Children-Christmas/dp/B075YT4RX3/ref=zg_bs_2237480011_62?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J took longer than 20.0 seconds..
2018-10-05 05:49:25 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Trend-Debut-Stroller-Cascade/dp/B078227XTG/ref=zg_bs_166845011_68?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Trend-Debut-Stroller-Cascade/dp/B078227XTG/ref=zg_bs_166845011_68?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM took longer than 20.0 seconds..
2018-10-05 05:49:37 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Babyzen-YOYO-Stroller-Black-Grey/dp/B01F8NS1U4/ref=zg_bs_166845011_79?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Babyzen-YOYO-Stroller-Black-Grey/dp/B01F8NS1U4/ref=zg_bs_166845011_79?_encoding=UTF8&psc=1&refRID=D72KKD7JYNKXZKD1XXMM took longer than 20.0 seconds..
2018-10-05 05:49:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Giftgarden-Picture-Butterfly-Edging-Framing/dp/B01H3BM696/ref=zg_bs_239231011_97?_encoding=UTF8&refRID=21YA8TXDBBCWB55CTQAT>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Giftgarden-Picture-Butterfly-Edging-Framing/dp/B01H3BM696/ref=zg_bs_239231011_97?_encoding=UTF8&refRID=21YA8TXDBBCWB55CTQAT took longer than 20.0 seconds..
2018-10-05 05:49:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Classic-Gameboy-Bank-Money-Box/dp/B06X6KHF5H/ref=zg_bs_2237480011_97?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Classic-Gameboy-Bank-Money-Box/dp/B06X6KHF5H/ref=zg_bs_2237480011_97?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J took longer than 20.0 seconds..
2018-10-05 05:49:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Jogger-2016-Double-Stroller/dp/B01BMYG1KK/ref=zg_bs_166846011_92?_encoding=UTF8&psc=1&refRID=CT3192E8A8VGX9J39QY5>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Jogger-2016-Double-Stroller/dp/B01BMYG1KK/ref=zg_bs_166846011_92?_encoding=UTF8&psc=1&refRID=CT3192E8A8VGX9J39QY5 took longer than 20.0 seconds..
2018-10-05 05:49:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Child-Cherish-Ceramic-Polka-Elephant/dp/B00T63RL2C/ref=zg_bs_2237480011_86?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Child-Cherish-Ceramic-Polka-Elephant/dp/B00T63RL2C/ref=zg_bs_2237480011_86?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J took longer than 20.0 seconds..
2018-10-05 05:49:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Sopu-Super-Money-Paper-Playing/dp/B07FYYHP3M/ref=zg_bs_2237480011_58?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Sopu-Super-Money-Paper-Playing/dp/B07FYYHP3M/ref=zg_bs_2237480011_58?_encoding=UTF8&psc=1&refRID=5MJA9X5GTYDYX44SPQ5J took longer than 20.0 seconds..
2018-10-05 05:49:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Handprint-Footprint-Registry-Keepsake-Decoration/dp/B07B67J7RZ/ref=zg_bs_239228011_81?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Handprint-Footprint-Registry-Keepsake-Decoration/dp/B07B67J7RZ/ref=zg_bs_239228011_81?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q took longer than 20.0 seconds..
2018-10-05 05:50:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Baby-Sensory-Security-Teething-Blanket/dp/B01LYNFFFE/ref=zg_bs_239228011_99?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Baby-Sensory-Security-Teething-Blanket/dp/B01LYNFFFE/ref=zg_bs_239228011_99?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q took longer than 20.0 seconds..
2018-10-05 05:50:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Little-Blossoms-Pearhead-Welcome-Keepsake/dp/B01M0ETI04/ref=zg_bs_239231011_69?_encoding=UTF8&psc=1&refRID=21YA8TXDBBCWB55CTQAT>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Little-Blossoms-Pearhead-Welcome-Keepsake/dp/B01M0ETI04/ref=zg_bs_239231011_69?_encoding=UTF8&psc=1&refRID=21YA8TXDBBCWB55CTQAT took longer than 20.0 seconds..
2018-10-05 05:50:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Arch-Kit-Decoration-Christmas-Graduation/dp/B07BXXRH92/ref=zg_bs_239229011_100?_encoding=UTF8&psc=1&refRID=9C7Q75CSHTK0KADDC38K>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Arch-Kit-Decoration-Christmas-Graduation/dp/B07BXXRH92/ref=zg_bs_239229011_100?_encoding=UTF8&psc=1&refRID=9C7Q75CSHTK0KADDC38K took longer than 20.0 seconds..
2018-10-05 05:50:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Nano-Towels-Drying-Twisty-Natural/dp/B01MXJ2MDR/ref=zg_bs_11056491_35?_encoding=UTF8&psc=1&refRID=BQS7AZ98AFX8Q914TV7V>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Nano-Towels-Drying-Twisty-Natural/dp/B01MXJ2MDR/ref=zg_bs_11056491_35?_encoding=UTF8&psc=1&refRID=BQS7AZ98AFX8Q914TV7V took longer than 20.0 seconds..
2018-10-05 05:50:20 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Personalized-Story-Book-Dinkleboo-Children/dp/B079YZGLB4/ref=zg_bs_239228011_97?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Personalized-Story-Book-Dinkleboo-Children/dp/B079YZGLB4/ref=zg_bs_239228011_97?_encoding=UTF8&psc=1&refRID=ABYZ000HAMCWNA9C1V7Q took longer than 20.0 seconds..
2018-10-05 05:50:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Cosmetic-Magnification-Tri-Fold-Magnifying-Countertop/dp/B076BS9CHP/ref=zg_bs_11063411_79?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Cosmetic-Magnification-Tri-Fold-Magnifying-Countertop/dp/B076BS9CHP/ref=zg_bs_11063411_79?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F took longer than 20.0 seconds..
2018-10-05 05:50:31 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/TcaFmac-Decorative-Storage-Containers-Organizing/dp/B0732VRB9C/ref=zg_bs_394314011_97?_encoding=UTF8&psc=1&refRID=AHJ5CYZRK7MKKVEVXAEX>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/TcaFmac-Decorative-Storage-Containers-Organizing/dp/B0732VRB9C/ref=zg_bs_394314011_97?_encoding=UTF8&psc=1&refRID=AHJ5CYZRK7MKKVEVXAEX took longer than 20.0 seconds..
2018-10-05 05:50:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Loofah-Charcoal-Sponge-Shower-Bouquet/dp/B07CHYS8P2/ref=zg_bs_11056491_90?_encoding=UTF8&psc=1&refRID=N8ECXD42HQGH4WZAZ836>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Loofah-Charcoal-Sponge-Shower-Bouquet/dp/B07CHYS8P2/ref=zg_bs_11056491_90?_encoding=UTF8&psc=1&refRID=N8ECXD42HQGH4WZAZ836 took longer than 20.0 seconds..
2018-10-05 05:50:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/JiBen-Magnifying-Diffused-Rotating-Adjustable/dp/B01HFNBAL2/ref=zg_bs_11063411_87?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/JiBen-Magnifying-Diffused-Rotating-Adjustable/dp/B01HFNBAL2/ref=zg_bs_11063411_87?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F took longer than 20.0 seconds..
2018-10-05 05:50:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Indian-Elephant-Compact-Mirror-Fashioncraft/dp/B00WGKIPHM/ref=zg_bs_11063411_97?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Indian-Elephant-Compact-Mirror-Fashioncraft/dp/B00WGKIPHM/ref=zg_bs_11063411_97?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F took longer than 20.0 seconds..
2018-10-05 05:50:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Classic-Hydraulic-Barber-Styling-Beauty/dp/B004GGFQB2/ref=zg_bs_15144566011_78?_encoding=UTF8&psc=1&refRID=SXMFD51RH7K0V40F9HBW>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Classic-Hydraulic-Barber-Styling-Beauty/dp/B004GGFQB2/ref=zg_bs_15144566011_78?_encoding=UTF8&psc=1&refRID=SXMFD51RH7K0V40F9HBW took longer than 20.0 seconds..
2018-10-05 05:50:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Adjustable-Hydraulic-Rolling-Swivel-Massage/dp/B018JCALBI/ref=zg_bs_15144566011_85?_encoding=UTF8&psc=1&refRID=SXMFD51RH7K0V40F9HBW>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Adjustable-Hydraulic-Rolling-Swivel-Massage/dp/B018JCALBI/ref=zg_bs_15144566011_85?_encoding=UTF8&psc=1&refRID=SXMFD51RH7K0V40F9HBW took longer than 20.0 seconds..
2018-10-05 05:50:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Excelvan-Magnification-Double-Sided-Extension-Polished/dp/B00SM4S7QK/ref=zg_bs_11063411_88?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Excelvan-Magnification-Double-Sided-Extension-Polished/dp/B00SM4S7QK/ref=zg_bs_11063411_88?_encoding=UTF8&psc=1&refRID=037GFEY92426YCDY6M8F took longer than 20.0 seconds..
2018-10-05 05:50:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Eastrin-Blending-Cosmetic-Concealer-Contouring/dp/B01MCQDFW4/ref=zg_bs_11059391_57?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Eastrin-Blending-Cosmetic-Concealer-Contouring/dp/B01MCQDFW4/ref=zg_bs_11059391_57?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D took longer than 20.0 seconds..
2018-10-05 05:51:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Makeup-Brush-Cleaner-Pro-Starter/dp/B06XFQYPQM/ref=zg_bs_11059391_84?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Makeup-Brush-Cleaner-Pro-Starter/dp/B06XFQYPQM/ref=zg_bs_11059391_84?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D took longer than 20.0 seconds..
2018-10-05 05:51:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/EmaxDesign-Sponges-Foundation-Blending-Concealer/dp/B07D6B8RJY/ref=zg_bs_11059391_96?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/EmaxDesign-Sponges-Foundation-Blending-Concealer/dp/B07D6B8RJY/ref=zg_bs_11059391_96?_encoding=UTF8&psc=1&refRID=1V8D31P6H7NBX9BDTB7D took longer than 20.0 seconds..
2018-10-05 05:51:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Xbes-professional-Cutting-Umbrella-Hairdressing/dp/B06XD7T64J/ref=zg_bs_10676449011_89?_encoding=UTF8&psc=1&refRID=GWTAD24357YWJGKSPSRX>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Xbes-professional-Cutting-Umbrella-Hairdressing/dp/B06XD7T64J/ref=zg_bs_10676449011_89?_encoding=UTF8&psc=1&refRID=GWTAD24357YWJGKSPSRX took longer than 20.0 seconds..
2018-10-05 05:51:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/ACURE-Essentials-Marula-Packaging-Vary/dp/B00BG238AU/ref=zg_bs_10666241011_92?_encoding=UTF8&psc=1&refRID=27BABT1R64EEMXESZPW9>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/ACURE-Essentials-Marula-Packaging-Vary/dp/B00BG238AU/ref=zg_bs_10666241011_92?_encoding=UTF8&psc=1&refRID=27BABT1R64EEMXESZPW9 took longer than 20.0 seconds..
2018-10-05 05:51:15 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Revlon-Detangle-Smooth-Cushion-Brush/dp/B01LPO5LCQ/ref=zg_bs_11058091_97?_encoding=UTF8&psc=1&refRID=512ZNGN30N8M8HQ7ZK4C>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Revlon-Detangle-Smooth-Cushion-Brush/dp/B01LPO5LCQ/ref=zg_bs_11058091_97?_encoding=UTF8&psc=1&refRID=512ZNGN30N8M8HQ7ZK4C took longer than 20.0 seconds..
2018-10-05 05:51:16 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Crease-Ponytail-Holders-Available-Quantities/dp/B00MTZUPW2/ref=zg_bs_11057971_98?_encoding=UTF8&psc=1&refRID=5TZB2PY0GCHW9ZRSH3WQ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Crease-Ponytail-Holders-Available-Quantities/dp/B00MTZUPW2/ref=zg_bs_11057971_98?_encoding=UTF8&psc=1&refRID=5TZB2PY0GCHW9ZRSH3WQ took longer than 20.0 seconds..
2018-10-05 05:51:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Tropic-Isle-Living-Jamaican-Castor/dp/B00FM6JHOC/ref=zg_bs_10666241011_90?_encoding=UTF8&psc=1&refRID=27BABT1R64EEMXESZPW9>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Tropic-Isle-Living-Jamaican-Castor/dp/B00FM6JHOC/ref=zg_bs_10666241011_90?_encoding=UTF8&psc=1&refRID=27BABT1R64EEMXESZPW9 took longer than 20.0 seconds..
2018-10-05 05:51:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Brittny-Professionals-Wig-Brush-Combo/dp/B005HFYOP0/ref=zg_bs_11058091_95?_encoding=UTF8&psc=1&refRID=8KWK9TESF1KGECP13PTB>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Brittny-Professionals-Wig-Brush-Combo/dp/B005HFYOP0/ref=zg_bs_11058091_95?_encoding=UTF8&psc=1&refRID=8KWK9TESF1KGECP13PTB took longer than 20.0 seconds..
2018-10-05 05:51:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/PrettyDate-Elastic-Nylon-Neutral-Beige/dp/B073B19W4V/ref=zg_bs_13105931_91?_encoding=UTF8&psc=1&refRID=ZX5Q8Y7Z74GKPPRVN4VD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/PrettyDate-Elastic-Nylon-Neutral-Beige/dp/B073B19W4V/ref=zg_bs_13105931_91?_encoding=UTF8&psc=1&refRID=ZX5Q8Y7Z74GKPPRVN4VD took longer than 20.0 seconds..
2018-10-05 05:51:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Topcosplay-Cosplay-Halloween-Costume-Braids/dp/B074M4X8N3/ref=zg_bs_13105931_80?_encoding=UTF8&psc=1&refRID=ZX5Q8Y7Z74GKPPRVN4VD>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Topcosplay-Cosplay-Halloween-Costume-Braids/dp/B074M4X8N3/ref=zg_bs_13105931_80?_encoding=UTF8&psc=1&refRID=ZX5Q8Y7Z74GKPPRVN4VD took longer than 20.0 seconds..
2018-10-05 05:52:06 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Growth-Essence-Liquid-Women-Thicken/dp/B07FKZ3YGH/ref=zg_bs_10898755011_68?_encoding=UTF8&psc=1&refRID=D5WQ6YA24B3Y84V428Z9>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Growth-Essence-Liquid-Women-Thicken/dp/B07FKZ3YGH/ref=zg_bs_10898755011_68?_encoding=UTF8&psc=1&refRID=D5WQ6YA24B3Y84V428Z9 took longer than 20.0 seconds..
2018-10-05 05:52:08 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Pro-Foilsheets-500s-10-75-Count/dp/B00LSV7ZTE/ref=zg_bs_11057451_93?_encoding=UTF8&psc=1&refRID=BWS328JZQZNWVT2YD7N9>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Pro-Foilsheets-500s-10-75-Count/dp/B00LSV7ZTE/ref=zg_bs_11057451_93?_encoding=UTF8&psc=1&refRID=BWS328JZQZNWVT2YD7N9 took longer than 20.0 seconds..
2018-10-05 05:52:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/HOLIDAY-COLLECTION-Fragrance-Sampler-Collection/dp/B01N9HLYS1/ref=zg_bs_11056891_88?_encoding=UTF8&psc=1&refRID=TFTQHCGJF1DQDXW17YTV>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/HOLIDAY-COLLECTION-Fragrance-Sampler-Collection/dp/B01N9HLYS1/ref=zg_bs_11056891_88?_encoding=UTF8&psc=1&refRID=TFTQHCGJF1DQDXW17YTV took longer than 20.0 seconds..
2018-10-05 05:52:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Marc-Jacobs-Daisy-Dream-Toilette/dp/B00MRR0YJQ/ref=zg_bs_11056931_91?_encoding=UTF8&psc=1&refRID=FCA3EMG1MQ40SPHXGDV5>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Marc-Jacobs-Daisy-Dream-Toilette/dp/B00MRR0YJQ/ref=zg_bs_11056931_91?_encoding=UTF8&psc=1&refRID=FCA3EMG1MQ40SPHXGDV5 took longer than 20.0 seconds..
2018-10-05 05:52:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Anxiety-Relief-Essential-Blend-10ml/dp/B01HUZIBCG/ref=zg_bs_3767491_99?_encoding=UTF8&psc=1&refRID=Q21F0JB82YER0X6CXMXQ>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Anxiety-Relief-Essential-Blend-10ml/dp/B01HUZIBCG/ref=zg_bs_3767491_99?_encoding=UTF8&psc=1&refRID=Q21F0JB82YER0X6CXMXQ took longer than 20.0 seconds..
2018-10-05 05:52:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.amazon.com/Hotel-Aroma-AromaTech-Scent-Diffusers/dp/B074B5WV4T/ref=zg_bs_7240192011_59?_encoding=UTF8&psc=1&refRID=DDH0AY6N7M3KS2KTDKGB>
Traceback (most recent call last):
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/lib/python3.5/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/usr/lib/python3.5/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/usr/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.amazon.com/Hotel-Aroma-AromaTech-Scent-Diffusers/dp/B074B5WV4T/ref=zg_bs_7240192011_59?_encoding=UTF8&psc=1&refRID=DDH0AY6N7M3KS2KTDKGB took longer than 20.0 seconds..
2018-10-05 07:02:27 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.amazon.com/Ballet-Tights-Girls-Leotards-Footed/dp/B07FC5GL7W/ref=zg_bs_2474938011_81?_encoding=UTF8&refRID=VM5XF5TMD8N7BQGZ1V4M. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
